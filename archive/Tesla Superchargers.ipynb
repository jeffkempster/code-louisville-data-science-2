{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tesla Superchargers\n",
    "\n",
    "\n",
    "## Possible Questions\n",
    "* Has the installation of chargers kept up with car production.\n",
    "\n",
    "Make sure the data set has numbers and not just text. See if it has value.\n",
    "\n",
    "Or maybe I can add data to the set with other datasets. \n",
    "\n",
    "apply thing from assignment 1, 2, 3 \n",
    "\n",
    "maybe add population numbers to the dataset based on lat/lng\n",
    "maybe with zip code level data. \n",
    "\n",
    "has chargers kept up with production of electic vehicles. \n",
    "\n",
    "Find the url to the json file at tesla,\n",
    "but keep the real json file incase it goes away\n",
    "\n",
    "web scrape the charge point database. \n",
    "\n",
    "median income with a certain distance of each charger or maybe by zip code\n",
    "\n",
    "forecasting, classification, correlation between multiple variables. \n",
    "\n",
    "\n",
    " pair this data with other population and other factors and other zip code level info\n",
    " \n",
    "TODO\n",
    "* clean the data set first\n",
    "* limit to US\n",
    "* clean the state names\n",
    "* find the kW Type if available\n",
    "* clean the zip code\n",
    "  * if zip code is missing can I get it from lat/lon\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns List: Index(['address_line_1', 'address_line_2', 'address_notes', 'address',\n",
      "       'amenities', 'baidu_lat', 'baidu_lng', 'chargers', 'city',\n",
      "       'common_name', 'country', 'destination_charger_logo',\n",
      "       'destination_website', 'directions_link', 'emails', 'geocode', 'hours',\n",
      "       'is_gallery', 'kiosk_pin_x', 'kiosk_pin_y', 'kiosk_zoom_pin_x',\n",
      "       'kiosk_zoom_pin_y', 'latitude', 'location_id', 'location_type',\n",
      "       'longitude', 'nid', 'open_soon', 'path', 'postal_code',\n",
      "       'province_state', 'region', 'sales_phone', 'sales_representative',\n",
      "       'sub_region', 'title', 'trt_id'],\n",
      "      dtype='object')\n",
      "(15737, 37)\n",
      "Rows: 15737, Columns: 37\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "chargers_df = pd.read_json('tesla-chargers.json')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "print(f\"Columns List: {chargers_df.columns}\")\n",
    "print(chargers_df.shape)\n",
    "print(f\"Rows: {chargers_df.shape[0]}, Columns: {chargers_df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shows the count of nulls in each field\n",
    "chargers_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will just show the null values in address in chargers_df\n",
    "chargers_df[chargers_df.address.isnull()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will drop any rows where address and province_state are null 'all'\n",
    "# or either are null 'any'\n",
    "#chargers_df.dropna(subset=['address', 'province_state'], how='any').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the unique values in postal_code \n",
    "# note that the top is just blank\n",
    "# you can drop_na = False to count the NaN also\n",
    "# in this case there aren't any so its the same counts\n",
    "chargers_df['hours'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chargers_us_df = chargers_df.query(\"country == 'United States'\")\n",
    "#chargers_us_df['postal_code'].value_counts(dropna=False)\n",
    "\n",
    "len(chargers_us_df)\n",
    "chargers_us_df.head()\n",
    "#chargers_us_df.to_csv('us_chargers.csv')\n",
    "# intersting that there is data as CA and data as California\n",
    "# I am going to need to map CA to California.\n",
    "# see if i can get a dataset to do this \n",
    "#chargers_us_df['province_state'].value_counts(dropna=False).head(10)\n",
    "\n",
    "#state_codes = pd.read_csv('us_state_codes.csv')\n",
    "#state_codes.head()\n",
    "#\n",
    "#chargers_us_df = pd.merge(chargers_us_df, state_codes, how=\"left\", left_on='province_state', right_on='Code')\n",
    "#\n",
    "#display(chargers_us_df[['province_state', 'Code', 'State']])\n",
    "#chargers_us_df['State'].value_counts(dropna=False)\n",
    "\n",
    "# this didnt quite work. I need to change CA to California and then count\n",
    "# the above changed just the CA to Califonia, but did not combine with the \n",
    "# existing word Califonia in the province_state\n",
    "# maybe just search and replace would be better, but I would still want to \n",
    "# do it from the csv file. I dont think it will be a problem, but I need \n",
    "# to search the entire field for CA, not just part, otherwise I will get\n",
    "# californialifornia\n",
    "\n",
    "# map to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling na values with a string\n",
    "chargers_df['province_state'].fillna(value = \"Unknown\", inplace = True)\n",
    "# more counts of province state\n",
    "chargers_df['province_state'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of superchargers in KY. \n",
    "\n",
    "# this did not work until I switched regex=False ðŸ¤¬\n",
    "q = \"country == 'United States' & province_state == 'KY' & location_type.str.contains('supercharger', regex=False)\"\n",
    "q2 = \"country == 'United States' & location_type.str.contains('supercharger', regex=False)\"\n",
    "us_df = chargers_df.query(q)\n",
    "print(len(us_df))\n",
    "\n",
    "us_df = us_df[['address_line_1',  'chargers', 'city', 'common_name', 'country', 'geocode', 'hours', 'latitude', 'location_id', 'location_type', 'longitude', 'nid', 'open_soon', 'postal_code', 'province_state', 'region', 'title', 'trt_id']]\n",
    "us_df.to_csv('tesla_json_us_superchargers.csv')\n",
    "#us_df[['address_line_1', 'location_type']].dtypes\n",
    "#print(f\"Columns List: {chargers_df.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_df2 = chargers_df[(chargers_df['country'] == 'United States') & (chargers_df['location_type'].str.contains('supercharger'))]\n",
    "us_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_df[['address','location_type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this may change the number format from scientific to a normal number\n",
    "# ax.ticklabel_format(useOffset=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
